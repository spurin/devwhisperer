# ğŸ§  Dev Whisperer â€” Your Local Dev Copilot

Welcome to **Dev Whisperer** â€” a lightweight, private, and blazing-fast coding assistant that lives on your machine and *never phones home*. Whether you're debugging spaghetti, refactoring your sins, or just want someone to explain what your past self wrote at 2am... **Dev Whisperer's got your back**.

---

## ğŸš€ What Is It?

**Dev Whisperer** is a single-page app powered by a locally-hosted large language model (LLM). It gives you **AI-powered code assistance** with **zero data leakage**.

- ğŸ§  Explain what code does  
- ğŸ” Suggest improvements or optimizations  
- ğŸ”§ Refactor code for clarity, performance, or best practices  
- ğŸ”’ 100% local â€” no internet required  
- âš¡ Works with any OpenAI-compatible LLM backend

---

## ğŸ› ï¸ Features

- âœï¸ Paste in your code and choose your desired action  
- ğŸ§™ Get smart, context-aware suggestions in real time  
- ğŸ’» Dockerized for simplicity  
- âš™ï¸ Environment configurable via `.env`

---

## ğŸ“¦ Quick Start (via Docker Desktop & Docker Compose)

### 1. Clone this repo

```bash
git clone https://github.com/yourname/devwhisperer.git
cd devwhisperer
```

Modify compose.yaml, if you wish to ue a different model

### 2. Run the app

```bash
docker compose up --build
```

Then open your browser at [http://localhost:3000](http://localhost:3000)  
You're ready to **pair-program with a local AI assistant**. âœ¨

---

## ğŸ¤– Compatible Models

You can use **any OpenAI-compatible chat model**, such as:

- `llama3.2` (highly recommended)
- `codellama:7b-instruct` (great for general code help)  
- `deepseek-coder`  
- `starcoder`, `mistral`, `phi`, etc.  

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ index.template.html   # HTML with placeholders for env vars
â”œâ”€â”€ nginx.conf.template   # Nginx config with dynamic model routing
â”œâ”€â”€ entrypoint.sh         # Replaces env vars at container start
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ compose.yaml
â””â”€â”€ .env                  # Your config
```

---

## ğŸ” Why Local?

Because your code is **your code**.  
No cloud calls, no vendor lock-in, no token limits. Just raw, local compute and real-time help.

---
